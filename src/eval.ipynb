{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b127b2b",
   "metadata": {},
   "source": [
    "## 1. Importacion De Librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8b32f",
   "metadata": {},
   "source": [
    "Se importan las librerías necesarias para la evaluación de modelos: numpy y pandas para manipulación de datos, scikit-learn para modelos y métricas, joblib para cargar modelos guardados, y os para manejo de rutas de archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías para manipulación de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelos de clasificación (solo para referencia de tipos)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Manejo de rutas y archivos\n",
    "import os\n",
    "\n",
    "# Carga y guardado de modelos\n",
    "import joblib\n",
    "\n",
    "# Métricas de evaluación\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08701f11",
   "metadata": {},
   "source": [
    "## 2. Definicion De Datos, Modelos y Direcrtorios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01d002",
   "metadata": {},
   "source": [
    "Se definen las constantes del proyecto: los 9 tipos de conjuntos de datos a evaluar (original, estandarizado, normalizado y sus variantes con PCA), los 4 modelos de clasificación (KNN, SVM, Naive Bayes, Random Forest), el número de folds para validación cruzada (5), y las rutas a los directorios de datos y modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ebb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de tipos de conjuntos de datos a evaluar\n",
    "DATASETS_DIRS = [\n",
    "    \"estandarizado\",\n",
    "    \"estandarizado_PCA80\",\n",
    "    \"estandarizado_PCA95\",\n",
    "    \"normalizado\",\n",
    "    \"normalizado_PCA80\",\n",
    "    \"normalizado_PCA95\",\n",
    "    \"original\",\n",
    "    \"original_PCA80\",\n",
    "    \"original_PCA95\",\n",
    "]\n",
    "\n",
    "# Nombres de los modelos a evaluar\n",
    "MODELS = [\n",
    "    \"KNN\",\n",
    "    \"SVM\",\n",
    "    \"NaiveBayes\",\n",
    "    \"RandomForest\"\n",
    "]\n",
    "\n",
    "# Número de folds en validación cruzada\n",
    "K_FOLDS = 5\n",
    "\n",
    "# Ruta al directorio de datos de prueba\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise FileNotFoundError(f\"Data directory '{DATA_DIR}' does not exist. Please ensure the data is available.\")\n",
    "\n",
    "# Ruta al directorio donde están guardados los modelos entrenados\n",
    "MODELS_DIR = os.path.join(\"..\", \"models\")\n",
    "\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    raise FileNotFoundError(f\"Models directory '{MODELS_DIR}' does not exist. Please ensure you execute the training script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c21c1d",
   "metadata": {},
   "source": [
    "## 3. Carga De Modelos Y Evaluacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd2676",
   "metadata": {},
   "source": [
    "Esta sección carga los modelos entrenados desde disco y los evalúa con los datos de prueba. Para cada combinación de tipo de dato, fold y modelo, se calculan múltiples métricas de rendimiento (accuracy, precision, recall, F1-score, ROC-AUC, sensitivity, specificity, FPR, FNR) usando estrategia macro-averaging. Los resultados se guardan en CSV: uno con predicciones y probabilidades, otro con las métricas calculadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf0eb6",
   "metadata": {},
   "source": [
    "### Funciones Auxiliares\n",
    "\n",
    "**`load_model(data_type, model_name, model_iteration, model_dir)`**  \n",
    "Deserializa modelos entrenados desde archivos `.pkl` usando joblib. Construye la ruta jerárquica: `models/{data_type}/{model_name}/model_fold_{iteration}.pkl`\n",
    "\n",
    "**`test_model_and_save_results(X_test, y_test, model)`**  \n",
    "Función principal de evaluación que:\n",
    "- Genera predicciones de clase (`predict`) y probabilidades (`predict_proba`)\n",
    "- Calcula métricas estándar de scikit-learn con `average='macro'` (media aritmética simple entre clases)\n",
    "- Integra métricas personalizadas de `calculate_multiclass_metrics`\n",
    "- Retorna dos DataFrames: uno con métricas agregadas y otro con predicciones individuales + probabilidades por clase\n",
    "\n",
    "**`calculate_multiclass_metrics(y_true, y_pred)`**  \n",
    "Implementa estrategia **One-vs-Rest (OvR)** para clasificación multiclase:\n",
    "La Estrategia consiste en ir calculando los FP, FN por cada clase de manera que el resto de clases se juntan en una haciendo del problema multi-class un problema binario.\n",
    "- Para cada clase $c$, binariza el problema: clase $c$ vs resto\n",
    "- Calcula matriz de confusión binaria (TP, TN, FP, FN)\n",
    "- Computa métricas por clase: Sensitivity (TPR), Specificity (TNR), FPR, FNR\n",
    "- Retorna **macro-averaging**: $\\text{metric} = \\frac{1}{K}\\sum_{i=1}^{K} \\text{metric}_i$ donde $K$ es el número de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(data_type, model_name, model_iteration, model_dir):\n",
    "    \"\"\"Carga un modelo entrenado desde disco\"\"\"\n",
    "    model_path = os.path.join(model_dir, f\"{data_type}\", f\"{model_name}\", f\"model_fold_{model_iteration}.pkl\")\n",
    "    model = joblib.load(model_path)\n",
    "    return model\n",
    "\n",
    "def test_model_and_save_results(X_test, y_test, model):\n",
    "    \"\"\"Evalúa el modelo y calcula métricas de rendimiento\"\"\"\n",
    "    \n",
    "    # Predice etiquetas y probabilidades\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Métricas básicas\n",
    "    metrics['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Métricas con promedio macro (trata todas las clases por igual)\n",
    "    metrics['precision'] = precision_score(y_test, y_pred, average='macro')\n",
    "    metrics['recall'] = recall_score(y_test, y_pred, average='macro')\n",
    "    metrics['f1_score'] = f1_score(y_test, y_pred, average='macro')\n",
    "    metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba, average='macro', multi_class='ovr')\n",
    "    \n",
    "    # Métricas multiclase personalizadas\n",
    "    multiclass_metrics = calculate_multiclass_metrics(y_test, y_pred)\n",
    "    \n",
    "    metrics.update(multiclass_metrics)\n",
    "    \n",
    "    # DataFrame con etiquetas verdaderas, predicciones y probabilidades por clase\n",
    "    predicted_probability_df = pd.DataFrame(y_pred_proba, columns=[f'Prob_Class_{i}' for i in range(y_pred_proba.shape[1])])\n",
    "    predicted_probability_df.insert(0, 'True_Label', y_test)\n",
    "    predicted_probability_df.insert(1, 'Predicted', y_pred)\n",
    "    \n",
    "    \n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    \n",
    "    return metrics_df, predicted_probability_df\n",
    "    \n",
    "    \n",
    "\n",
    "def calculate_multiclass_metrics(y_true, y_pred):\n",
    "    \"\"\"Calcula métricas adicionales para clasificación multiclase usando estrategia One-vs-Rest\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    unique_classes = np.unique(y_true)\n",
    "    \n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    fpr_list = []\n",
    "    fnr_list = []\n",
    "    \n",
    "    # Para cada clase, calcula métricas binarias\n",
    "    for cls in unique_classes:\n",
    "        y_true_binary = (y_true == cls).astype(int)\n",
    "        y_pred_binary = (y_pred == cls).astype(int)\n",
    "        \n",
    "        # Matriz de confusión\n",
    "        TP = np.sum((y_true_binary == 1) & (y_pred_binary == 1))\n",
    "        TN = np.sum((y_true_binary == 0) & (y_pred_binary == 0))\n",
    "        FP = np.sum((y_true_binary == 0) & (y_pred_binary == 1))\n",
    "        FN = np.sum((y_true_binary == 1) & (y_pred_binary == 0))\n",
    "        \n",
    "        # Cálculo de métricas por clase\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0  # TPR, Recall\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0  # TNR\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0  # Tasa de falsos positivos\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 0  # Tasa de falsos negativos\n",
    "        \n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        fpr_list.append(fpr)\n",
    "        fnr_list.append(fnr)\n",
    "    \n",
    "    # Promedio macro de métricas\n",
    "    metrics['sensitivity'] = np.mean(sensitivity_list)\n",
    "    metrics['specificity'] = np.mean(specificity_list)\n",
    "    metrics['fpr'] = np.mean(fpr_list)\n",
    "    metrics['fnr'] = np.mean(fnr_list)\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "\n",
    "# Loop principal de evaluación: itera sobre tipos de datos, folds y modelos\n",
    "for data_type in DATASETS_DIRS:\n",
    "    \n",
    "    data_path = os.path.join(DATA_DIR, data_type)\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data path '{data_path}' does not exist. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    for fold in range(K_FOLDS):\n",
    "        \n",
    "        # Carga datos de prueba del fold actual\n",
    "        fold_path = os.path.join(data_path, f\"test_{fold + 1}_{data_type}.csv\")\n",
    "        \n",
    "        X_test = pd.read_csv( fold_path ).values[:, :-1]\n",
    "        y_test = pd.read_csv( fold_path ).values[:, -1]\n",
    "        \n",
    "        for model_name in MODELS:\n",
    "            \n",
    "            # Carga el modelo entrenado\n",
    "            model = load_model(data_type, model_name, fold + 1, MODELS_DIR)\n",
    "            \n",
    "            print(f\"Evaluating Model: {model_name}, Data Type: {data_type}, Fold: {fold + 1}\")\n",
    "            \n",
    "            # Evalúa y obtiene métricas\n",
    "            metrics_df, predicted_probability_df = test_model_and_save_results(X_test, y_test, model)\n",
    "            \n",
    "            # Prepara directorio de resultados\n",
    "            results_dir = os.path.join(\"..\", \"results\", data_type, model_name)\n",
    "            \n",
    "            if not os.path.exists(results_dir):\n",
    "                os.makedirs(results_dir)\n",
    "            \n",
    "            # Guarda predicciones y probabilidades\n",
    "            results_path = os.path.join(results_dir, f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "            predicted_probability_df.to_csv(results_path, index=False)\n",
    "            \n",
    "            print(f\"Results saved at: {results_path}\")\n",
    "            \n",
    "            # Guarda métricas\n",
    "            metrics_path = os.path.join(results_dir, f\"results_metrics_fold_{fold + 1}.csv\")\n",
    "            metrics_df.to_csv(metrics_path, index=False)\n",
    "            \n",
    "            print(f\"Metrics saved at: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d1244",
   "metadata": {},
   "source": [
    "## 4. Ensemble De Modelos Y Evaluacion Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc91b69",
   "metadata": {},
   "source": [
    "Se implementan tres técnicas de ensemble para combinar las predicciones de los 4 modelos base:\n",
    "\n",
    "1. **Votación**: Cada modelo vota por una clase y se elige la más votada (hard voting)\n",
    "2. **Media de probabilidades**: Se promedian las probabilidades de todos los modelos y se elige la clase con mayor probabilidad promedio (soft voting)\n",
    "3. **Mediana de probabilidades**: Similar a la media pero usando mediana, más robusto ante valores extremos\n",
    "\n",
    "Los resultados de cada técnica se guardan en directorios separados (Ensemble_Votacion, Ensemble_Media, Ensemble_Mediana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bba2045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Votacion\n",
    "\n",
    "for data_type in DATASETS_DIRS:\n",
    "    \n",
    "    path = os.path.join(\"..\", \"results\", data_type)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Results path '{path}' does not exist. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    for fold in range(K_FOLDS):\n",
    "        \n",
    "        knn_csv = os.path.join(path, \"KNN\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        svm_csv = os.path.join(path, \"SVM\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        nb_csv = os.path.join(path, \"NaiveBayes\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        rf_csv = os.path.join(path, \"RandomForest\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        \n",
    "        knn_df = pd.read_csv(knn_csv)\n",
    "        svm_df = pd.read_csv(svm_csv)\n",
    "        nb_df = pd.read_csv(nb_csv)\n",
    "        rf_df = pd.read_csv(rf_csv)\n",
    "        \n",
    "        true_labels = knn_df['True_Label'].values\n",
    "        \n",
    "        predictions = np.array([\n",
    "            knn_df['Predicted'].values,\n",
    "            svm_df['Predicted'].values,\n",
    "            nb_df['Predicted'].values,\n",
    "            rf_df['Predicted'].values\n",
    "        ]).astype(int)\n",
    "        \n",
    "        final_predictions = []\n",
    "        \n",
    "        for i in range(predictions.shape[1]):\n",
    "            counts = np.bincount(predictions[:, i])\n",
    "            final_pred = np.argmax(counts)\n",
    "            final_predictions.append(final_pred)\n",
    "        \n",
    "        votacion_df = pd.DataFrame({\n",
    "            'True_Label': true_labels,\n",
    "            'Predicted': final_predictions\n",
    "        })\n",
    "        \n",
    "        votacion_csv = os.path.join(path, \"Ensemble_Votacion\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        votacion_dir = os.path.dirname(votacion_csv)\n",
    "        \n",
    "        if not os.path.exists(votacion_dir):\n",
    "            os.makedirs(votacion_dir)\n",
    "    \n",
    "        votacion_df.to_csv(votacion_csv, index=False)\n",
    "        \n",
    "        \n",
    "# 2. Por Media De Probabilidades\n",
    "\n",
    "for data_type in DATASETS_DIRS:\n",
    "    \n",
    "    path = os.path.join(\"..\", \"results\", data_type)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Results path '{path}' does not exist. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    for fold in range(K_FOLDS):\n",
    "        \n",
    "        knn_csv = os.path.join(path, \"KNN\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        svm_csv = os.path.join(path, \"SVM\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        nb_csv = os.path.join(path, \"NaiveBayes\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        rf_csv = os.path.join(path, \"RandomForest\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        \n",
    "        knn_df = pd.read_csv(knn_csv)\n",
    "        svm_df = pd.read_csv(svm_csv)\n",
    "        nb_df = pd.read_csv(nb_csv)\n",
    "        rf_df = pd.read_csv(rf_csv)\n",
    "        \n",
    "        true_labels = knn_df['True_Label'].values\n",
    "        \n",
    "        probas = np.array([\n",
    "            knn_df.filter(like='Prob_Class_').values,\n",
    "            svm_df.filter(like='Prob_Class_').values,\n",
    "            nb_df.filter(like='Prob_Class_').values,\n",
    "            rf_df.filter(like='Prob_Class_').values\n",
    "        ])\n",
    "        \n",
    "        probas_media = np.mean(probas, axis=0)\n",
    "        \n",
    "        final_predictions = np.argmax(probas_media, axis=1)\n",
    "        \n",
    "        media_df = pd.DataFrame({\n",
    "            'True_Label': true_labels,\n",
    "            'Predicted': final_predictions,\n",
    "            **{f'Prob_Class_{i}': probas_media[:, i] for i in range(probas_media.shape[1])}\n",
    "        })\n",
    "        \n",
    "        media_csv = os.path.join(path, \"Ensemble_Media\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        media_dir = os.path.dirname(media_csv)\n",
    "        \n",
    "        if not os.path.exists(media_dir):\n",
    "            os.makedirs(media_dir)\n",
    "    \n",
    "        media_df.to_csv(media_csv, index=False)\n",
    "        \n",
    "        \n",
    "# 3. Por Mediana De Probabilidades\n",
    "\n",
    "for data_type in DATASETS_DIRS:\n",
    "    \n",
    "    path = os.path.join(\"..\", \"results\", data_type)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Results path '{path}' does not exist. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    for fold in range(K_FOLDS):\n",
    "        \n",
    "        knn_csv = os.path.join(path, \"KNN\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        svm_csv = os.path.join(path, \"SVM\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        nb_csv = os.path.join(path, \"NaiveBayes\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        rf_csv = os.path.join(path, \"RandomForest\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        \n",
    "        knn_df = pd.read_csv(knn_csv)\n",
    "        svm_df = pd.read_csv(svm_csv)\n",
    "        nb_df = pd.read_csv(nb_csv)\n",
    "        rf_df = pd.read_csv(rf_csv)\n",
    "        \n",
    "        true_labels = knn_df['True_Label'].values\n",
    "        \n",
    "        probas = np.array([\n",
    "            knn_df.filter(like='Prob_Class_').values,\n",
    "            svm_df.filter(like='Prob_Class_').values,\n",
    "            nb_df.filter(like='Prob_Class_').values,\n",
    "            rf_df.filter(like='Prob_Class_').values\n",
    "        ])\n",
    "        \n",
    "        probas_mediana = np.median(probas, axis=0)\n",
    "        \n",
    "        final_predictions = np.argmax(probas_mediana, axis=1)\n",
    "        \n",
    "        mediana_df = pd.DataFrame({\n",
    "            'True_Label': true_labels,\n",
    "            'Predicted': final_predictions,\n",
    "            **{f'Prob_Class_{i}': probas_mediana[:, i] for i in range(probas_mediana.shape[1])}\n",
    "        })\n",
    "        \n",
    "        mediana_csv = os.path.join(path, \"Ensemble_Mediana\", f\"results_predictions_fold_{fold + 1}.csv\")\n",
    "        mediana_dir = os.path.dirname(mediana_csv)\n",
    "        \n",
    "        if not os.path.exists(mediana_dir):\n",
    "            os.makedirs(mediana_dir)\n",
    "    \n",
    "        mediana_df.to_csv(mediana_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
